---
title: Quickstart
description: 'Hit the ground running with evaluation'
---

## Overview

Each evaluation of your AI feature comprises four components -
* **prompt template**, 
* **language model**
* **test set**
* **set of metrics that indicate whether the output was desirable or not**

On the Stupidly Simple Benchmarking Infrastructure (SSBI), the prompt template is interpolated with each set of prompt variables from the test set, run through the chosen language model and then evaluated on the defined metrics.
The metric values are then aggregated to provide a performance benchmark for this unique combination.

See [Running your first evaluation](#running-your-first-evaluation) to get started, or learn more about each component by clicking on a card below

<CardGroup cols={2}>
  <Card
    title="Prompt Templates"
    icon="palette"
    href="https://docs.withemissary.com/evaluation/prompt_template"
  >
    Explore prompt development on Emissary
  </Card>
  <Card
    title="Test Sets"
    icon="code"
    href="https://docs.withemissary.com/evaluation/test_set"
  >
Upload, generate and expand test sets
  </Card>
  <Card
    title="Language Models"
    icon="screwdriver-wrench"
    href="https://docs.withemissary.com/evaluation/models"
  >
Choose the engine that drives your AI
  </Card>
  <Card
    title="Metrics"
    icon="stars"
    href="https://docs.withemissary.com/evaluation/metrics"
  >
  Explore new ways to measure your AI's performance in production
  </Card>
</CardGroup>

## Running your first evaluation

**Step 1**: Enter your prompt template. Remember, any variables in the prompt, that is dynamic inputs - should be within curly braces as such `{variable}`. 
See below for an example: 
<Frame>
  <img src="/images/sample_prompt.png" style={{ borderRadius: '0.5rem' }} />
</Frame>
**Step 2**: Input your Open AI key 

**Step 3**: Upload or generate test set. Select any csv file you have, or click the 'Generate samples' button to get samples that correspond to the format needed for the prompt.
> _Note_: You must have already inputted your OpenAI API key and a prompt template in order to activate sample generation capability.

Ensure that your spreadsheet has a column corresponding to every single prompt variable. That is, there must be a column with the exact same name as each prompt variable in the csv file. 


**Step 4**: Select the models you want to evaluate on. You may select one or more models - and the entire model list from OpenAI is available to you.

<Frame>
  <img src="/images/fields.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

**Step 5**: Check the metrics you want to evaluate against. Choose from our preset common metrics, or better yet - define your own.

**Step 6**: Hit 'Run', and your corresponding outputs and chosen metrics will start being generated immediately.


<Frame>
  <img src="/images/fields.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

**Step 7**: For manually evaluated metrics, click the 'Rapid Evaluation' button below each evaluation spreadsheet to enter our RapidEval mode and score your outputs.
Your LLM-generated metrics will be present already and all your metrics, once available, will be auto-aggregated. Feeling _optimal_? Send over the spreadsheet to Emissary from the top right 'Send to Emissary' button and we'll label it for you!


<Frame>
  <img src="/images/rapid_eval.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

**Step 8**: Download the final spreadsheet, or share with your colleagues from the top right 'Share' button. You can also find your spreadsheets later within our 'Evaluations' Panel on your dashboard!

