---
title: Quickstart
description: 'Hit the ground running with evaluation'
---

## Overview

Each evaluation of your AI feature comprises four components -
* **prompt template**, 
* **language model**
* **test set**
* **set of metrics that indicate whether the output was desirable or not**

On the Absurdly Simple Benchmarking Infrastructure (SSBI), the prompt template is interpolated with each set of prompt variables from the test set, run through the chosen language model and then evaluated on the defined metrics.
The metric values are then aggregated to provide a performance benchmark for this unique combination.

See [Running your first evaluation](#running-your-first-evaluation) to get started, or learn more about each component by clicking on a card below

<CardGroup cols={2}>
  <Card
    title="Prompt Templates"
    icon="palette"
    href="https://docs.withemissary.com/evaluation/prompt_template"
  >
    Explore prompt development on Emissary
  </Card>
  <Card
    title="Test Sets"
    icon="code"
    href="https://docs.withemissary.com/evaluation/test_set"
  >
Upload, generate and expand test sets
  </Card>
  <Card
    title="Language Models"
    icon="screwdriver-wrench"
    href="https://docs.withemissary.com/evaluation/models"
  >
Choose the engine that drives your AI
  </Card>
  <Card
    title="Metrics"
    icon="stars"
    href="https://docs.withemissary.com/evaluation/metrics"
  >
  Explore new ways to measure your AI's performance in production
  </Card>
</CardGroup>

## Running your first evaluation

**Step 1**: Enter your prompt template. Remember, any variables in the prompt, that is dynamic inputs - should be within curly braces as such `{{variable}}`. 
See below for an example: 
<Frame>
  <img src="/images/evaluations/sample_prompt.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

**Step 2**: Select your evaluation mode - we enable evaluating on a single model, model migrations (one to another) or multi-model evalutions. 
<Frame>
  <img src="/images/evaluations/fields.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

**Step 3**: Based on your mode, select the models you want to evaluate on (one, two or many). The entire model lists from OpenAI and Anthropic, as well as Mistral models, are available for selection. 

**Step 4**: Insert API keys. You will always need to insert an OpenAI API key as we use this for LLM-based evaluations and sample generation (if you select them), but if you choose to evaluate a Gemini model, we will also ask for a corresponding API key for Gemini.
<Frame>
  <img src="/images/evaluations/apikeys.png" style={{ borderRadius: '0.5rem' }} />
</Frame>


**Step 5**: Upload or generate test set. Select any csv file you have, or click the 'Generate samples' button to get samples that correspond to the format needed for the prompt.
> _Note_: You must have already inputted your OpenAI API key and a prompt template in order to activate sample generation capability.

Ensure that your spreadsheet has a column corresponding to every single prompt variable. That is, there must be a column with the exact same name as each prompt variable in the csv file. 

**Step 6**: Check the metrics you want to evaluate against. Choose from our preset common metrics, or better yet - define your own by clicking on the '+' sign next to the last metric.

<Frame>
  <img src="/images/evaluations/metric.png" style={{ borderRadius: '0.5rem' }} />
</Frame>


**Step 7**: Hit 'Run', and your corresponding outputs and chosen metrics will start being generated immediately.


**Step 7**: For manually evaluated metrics, click the 'Rapid Evaluation' button below each evaluation spreadsheet to enter our RapidEval mode and score your outputs.
Your LLM-generated metrics will be present already and all your metrics, once available, will be auto-aggregated. Feeling _optimal_? Send over the spreadsheet to Emissary from the top right 'Send to Emissary' button and we'll label it for you!

<Frame>
  <img src="/images/rapid_eval.png" style={{ borderRadius: '0.5rem' }} />
</Frame>


<Frame>
  <img src="/images/evaluations/rapid_eval_interface.png" style={{ borderRadius: '0.5rem' }} />
</Frame>

**Step 8**: Download the final spreadsheet, or share with your colleagues from the top right 'Share' button. You can also find your spreadsheets later within our 'Evaluations' Panel on your dashboard!

